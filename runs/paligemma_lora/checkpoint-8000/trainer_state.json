{
  "best_global_step": 8000,
  "best_metric": 0.49997416185313626,
  "best_model_checkpoint": "runs/paligemma_lora/checkpoint-8000",
  "epoch": 0.9493858660179196,
  "eval_steps": 8000,
  "global_step": 8000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0023734646650447992,
      "grad_norm": 14.492666244506836,
      "learning_rate": 1.5019762845849802e-05,
      "loss": 2.8211,
      "step": 20
    },
    {
      "epoch": 0.0047469293300895984,
      "grad_norm": 5.612584590911865,
      "learning_rate": 3.08300395256917e-05,
      "loss": 0.5724,
      "step": 40
    },
    {
      "epoch": 0.007120393995134398,
      "grad_norm": 7.776487827301025,
      "learning_rate": 4.66403162055336e-05,
      "loss": 0.5057,
      "step": 60
    },
    {
      "epoch": 0.009493858660179197,
      "grad_norm": 5.572386741638184,
      "learning_rate": 6.245059288537549e-05,
      "loss": 0.429,
      "step": 80
    },
    {
      "epoch": 0.011867323325223996,
      "grad_norm": 5.473940372467041,
      "learning_rate": 7.82608695652174e-05,
      "loss": 0.4387,
      "step": 100
    },
    {
      "epoch": 0.014240787990268795,
      "grad_norm": 3.7595980167388916,
      "learning_rate": 9.407114624505929e-05,
      "loss": 0.4158,
      "step": 120
    },
    {
      "epoch": 0.016614252655313595,
      "grad_norm": 7.062931060791016,
      "learning_rate": 0.0001098814229249012,
      "loss": 0.4653,
      "step": 140
    },
    {
      "epoch": 0.018987717320358394,
      "grad_norm": 4.099606513977051,
      "learning_rate": 0.0001256916996047431,
      "loss": 0.4403,
      "step": 160
    },
    {
      "epoch": 0.021361181985403193,
      "grad_norm": 4.19439172744751,
      "learning_rate": 0.00014150197628458498,
      "loss": 0.4571,
      "step": 180
    },
    {
      "epoch": 0.023734646650447992,
      "grad_norm": 6.178771495819092,
      "learning_rate": 0.00015731225296442689,
      "loss": 0.4558,
      "step": 200
    },
    {
      "epoch": 0.02610811131549279,
      "grad_norm": 3.6871650218963623,
      "learning_rate": 0.0001731225296442688,
      "loss": 0.4505,
      "step": 220
    },
    {
      "epoch": 0.02848157598053759,
      "grad_norm": 5.517406940460205,
      "learning_rate": 0.00018893280632411067,
      "loss": 0.4293,
      "step": 240
    },
    {
      "epoch": 0.03085504064558239,
      "grad_norm": 4.544211387634277,
      "learning_rate": 0.00019985319305113776,
      "loss": 0.4249,
      "step": 260
    },
    {
      "epoch": 0.03322850531062719,
      "grad_norm": 3.4713821411132812,
      "learning_rate": 0.00019936383655493026,
      "loss": 0.4291,
      "step": 280
    },
    {
      "epoch": 0.035601969975671985,
      "grad_norm": 2.120205879211426,
      "learning_rate": 0.0001988744800587228,
      "loss": 0.4156,
      "step": 300
    },
    {
      "epoch": 0.03797543464071679,
      "grad_norm": 3.6487009525299072,
      "learning_rate": 0.00019838512356251531,
      "loss": 0.3948,
      "step": 320
    },
    {
      "epoch": 0.04034889930576158,
      "grad_norm": 4.892750263214111,
      "learning_rate": 0.0001978957670663078,
      "loss": 0.416,
      "step": 340
    },
    {
      "epoch": 0.042722363970806386,
      "grad_norm": 3.294347047805786,
      "learning_rate": 0.0001974064105701003,
      "loss": 0.4055,
      "step": 360
    },
    {
      "epoch": 0.04509582863585118,
      "grad_norm": 2.2664601802825928,
      "learning_rate": 0.00019691705407389284,
      "loss": 0.4228,
      "step": 380
    },
    {
      "epoch": 0.047469293300895984,
      "grad_norm": 3.0084991455078125,
      "learning_rate": 0.00019642769757768536,
      "loss": 0.4122,
      "step": 400
    },
    {
      "epoch": 0.04984275796594078,
      "grad_norm": 3.7070367336273193,
      "learning_rate": 0.00019593834108147786,
      "loss": 0.4344,
      "step": 420
    },
    {
      "epoch": 0.05221622263098558,
      "grad_norm": 1.8261351585388184,
      "learning_rate": 0.00019544898458527039,
      "loss": 0.404,
      "step": 440
    },
    {
      "epoch": 0.05458968729603038,
      "grad_norm": 3.950197696685791,
      "learning_rate": 0.00019495962808906288,
      "loss": 0.3971,
      "step": 460
    },
    {
      "epoch": 0.05696315196107518,
      "grad_norm": 4.091215133666992,
      "learning_rate": 0.0001944702715928554,
      "loss": 0.4318,
      "step": 480
    },
    {
      "epoch": 0.05933661662611998,
      "grad_norm": 5.311039447784424,
      "learning_rate": 0.0001939809150966479,
      "loss": 0.4267,
      "step": 500
    },
    {
      "epoch": 0.06171008129116478,
      "grad_norm": 4.000807285308838,
      "learning_rate": 0.00019349155860044043,
      "loss": 0.3972,
      "step": 520
    },
    {
      "epoch": 0.06408354595620958,
      "grad_norm": 3.1619060039520264,
      "learning_rate": 0.00019300220210423293,
      "loss": 0.3821,
      "step": 540
    },
    {
      "epoch": 0.06645701062125438,
      "grad_norm": 1.40880286693573,
      "learning_rate": 0.00019251284560802546,
      "loss": 0.4186,
      "step": 560
    },
    {
      "epoch": 0.06883047528629918,
      "grad_norm": 2.510444164276123,
      "learning_rate": 0.00019202348911181798,
      "loss": 0.4194,
      "step": 580
    },
    {
      "epoch": 0.07120393995134397,
      "grad_norm": 2.6113052368164062,
      "learning_rate": 0.00019153413261561048,
      "loss": 0.3803,
      "step": 600
    },
    {
      "epoch": 0.07357740461638877,
      "grad_norm": 2.26802659034729,
      "learning_rate": 0.00019104477611940298,
      "loss": 0.3801,
      "step": 620
    },
    {
      "epoch": 0.07595086928143358,
      "grad_norm": 2.530818462371826,
      "learning_rate": 0.0001905554196231955,
      "loss": 0.3696,
      "step": 640
    },
    {
      "epoch": 0.07832433394647838,
      "grad_norm": 1.3149033784866333,
      "learning_rate": 0.00019006606312698803,
      "loss": 0.3974,
      "step": 660
    },
    {
      "epoch": 0.08069779861152317,
      "grad_norm": 1.9251666069030762,
      "learning_rate": 0.00018957670663078053,
      "loss": 0.3917,
      "step": 680
    },
    {
      "epoch": 0.08307126327656797,
      "grad_norm": 1.948172926902771,
      "learning_rate": 0.00018908735013457303,
      "loss": 0.4046,
      "step": 700
    },
    {
      "epoch": 0.08544472794161277,
      "grad_norm": 1.6953295469284058,
      "learning_rate": 0.00018859799363836555,
      "loss": 0.4195,
      "step": 720
    },
    {
      "epoch": 0.08781819260665757,
      "grad_norm": 2.6756811141967773,
      "learning_rate": 0.00018810863714215808,
      "loss": 0.4348,
      "step": 740
    },
    {
      "epoch": 0.09019165727170236,
      "grad_norm": 2.8280749320983887,
      "learning_rate": 0.00018761928064595058,
      "loss": 0.3726,
      "step": 760
    },
    {
      "epoch": 0.09256512193674717,
      "grad_norm": 2.8297033309936523,
      "learning_rate": 0.0001871299241497431,
      "loss": 0.4149,
      "step": 780
    },
    {
      "epoch": 0.09493858660179197,
      "grad_norm": 1.7508517503738403,
      "learning_rate": 0.0001866405676535356,
      "loss": 0.371,
      "step": 800
    },
    {
      "epoch": 0.09731205126683677,
      "grad_norm": 2.4893953800201416,
      "learning_rate": 0.00018615121115732813,
      "loss": 0.4039,
      "step": 820
    },
    {
      "epoch": 0.09968551593188156,
      "grad_norm": 3.376779794692993,
      "learning_rate": 0.00018566185466112063,
      "loss": 0.4248,
      "step": 840
    },
    {
      "epoch": 0.10205898059692636,
      "grad_norm": 3.0770747661590576,
      "learning_rate": 0.00018517249816491315,
      "loss": 0.4123,
      "step": 860
    },
    {
      "epoch": 0.10443244526197117,
      "grad_norm": 3.4694344997406006,
      "learning_rate": 0.00018468314166870565,
      "loss": 0.412,
      "step": 880
    },
    {
      "epoch": 0.10680590992701597,
      "grad_norm": 2.7113873958587646,
      "learning_rate": 0.00018419378517249818,
      "loss": 0.4018,
      "step": 900
    },
    {
      "epoch": 0.10917937459206076,
      "grad_norm": 2.8620827198028564,
      "learning_rate": 0.0001837044286762907,
      "loss": 0.3798,
      "step": 920
    },
    {
      "epoch": 0.11155283925710556,
      "grad_norm": 1.1317744255065918,
      "learning_rate": 0.0001832150721800832,
      "loss": 0.3798,
      "step": 940
    },
    {
      "epoch": 0.11392630392215036,
      "grad_norm": 1.6614340543746948,
      "learning_rate": 0.0001827257156838757,
      "loss": 0.3943,
      "step": 960
    },
    {
      "epoch": 0.11629976858719515,
      "grad_norm": 1.69460129737854,
      "learning_rate": 0.00018223635918766822,
      "loss": 0.411,
      "step": 980
    },
    {
      "epoch": 0.11867323325223995,
      "grad_norm": 2.5196480751037598,
      "learning_rate": 0.00018174700269146075,
      "loss": 0.3864,
      "step": 1000
    },
    {
      "epoch": 0.12104669791728476,
      "grad_norm": 1.4873319864273071,
      "learning_rate": 0.00018125764619525325,
      "loss": 0.3749,
      "step": 1020
    },
    {
      "epoch": 0.12342016258232956,
      "grad_norm": 1.9708566665649414,
      "learning_rate": 0.00018076828969904575,
      "loss": 0.4184,
      "step": 1040
    },
    {
      "epoch": 0.12579362724737436,
      "grad_norm": 1.3966953754425049,
      "learning_rate": 0.00018027893320283827,
      "loss": 0.3646,
      "step": 1060
    },
    {
      "epoch": 0.12816709191241915,
      "grad_norm": 3.1370434761047363,
      "learning_rate": 0.0001797895767066308,
      "loss": 0.404,
      "step": 1080
    },
    {
      "epoch": 0.13054055657746394,
      "grad_norm": 2.0397770404815674,
      "learning_rate": 0.0001793002202104233,
      "loss": 0.3888,
      "step": 1100
    },
    {
      "epoch": 0.13291402124250876,
      "grad_norm": 2.417708158493042,
      "learning_rate": 0.0001788108637142158,
      "loss": 0.4101,
      "step": 1120
    },
    {
      "epoch": 0.13528748590755355,
      "grad_norm": 2.154984951019287,
      "learning_rate": 0.00017832150721800832,
      "loss": 0.3786,
      "step": 1140
    },
    {
      "epoch": 0.13766095057259836,
      "grad_norm": 2.0413460731506348,
      "learning_rate": 0.00017783215072180085,
      "loss": 0.3872,
      "step": 1160
    },
    {
      "epoch": 0.14003441523764315,
      "grad_norm": 2.273076295852661,
      "learning_rate": 0.00017734279422559334,
      "loss": 0.4027,
      "step": 1180
    },
    {
      "epoch": 0.14240787990268794,
      "grad_norm": 1.1938550472259521,
      "learning_rate": 0.00017685343772938587,
      "loss": 0.37,
      "step": 1200
    },
    {
      "epoch": 0.14478134456773276,
      "grad_norm": 2.567981481552124,
      "learning_rate": 0.00017636408123317837,
      "loss": 0.4207,
      "step": 1220
    },
    {
      "epoch": 0.14715480923277754,
      "grad_norm": 2.513493061065674,
      "learning_rate": 0.0001758747247369709,
      "loss": 0.3866,
      "step": 1240
    },
    {
      "epoch": 0.14952827389782233,
      "grad_norm": 3.228553056716919,
      "learning_rate": 0.00017538536824076342,
      "loss": 0.3927,
      "step": 1260
    },
    {
      "epoch": 0.15190173856286715,
      "grad_norm": 3.5048351287841797,
      "learning_rate": 0.00017489601174455592,
      "loss": 0.3971,
      "step": 1280
    },
    {
      "epoch": 0.15427520322791194,
      "grad_norm": 2.2240402698516846,
      "learning_rate": 0.00017440665524834842,
      "loss": 0.4,
      "step": 1300
    },
    {
      "epoch": 0.15664866789295676,
      "grad_norm": 1.3879131078720093,
      "learning_rate": 0.00017391729875214094,
      "loss": 0.3466,
      "step": 1320
    },
    {
      "epoch": 0.15902213255800154,
      "grad_norm": 2.8788251876831055,
      "learning_rate": 0.00017342794225593347,
      "loss": 0.3956,
      "step": 1340
    },
    {
      "epoch": 0.16139559722304633,
      "grad_norm": 3.356626033782959,
      "learning_rate": 0.00017293858575972597,
      "loss": 0.4054,
      "step": 1360
    },
    {
      "epoch": 0.16376906188809115,
      "grad_norm": 4.536192893981934,
      "learning_rate": 0.00017244922926351846,
      "loss": 0.43,
      "step": 1380
    },
    {
      "epoch": 0.16614252655313594,
      "grad_norm": 2.287463426589966,
      "learning_rate": 0.00017195987276731102,
      "loss": 0.3786,
      "step": 1400
    },
    {
      "epoch": 0.16851599121818073,
      "grad_norm": 1.401665449142456,
      "learning_rate": 0.00017147051627110351,
      "loss": 0.3734,
      "step": 1420
    },
    {
      "epoch": 0.17088945588322554,
      "grad_norm": 2.9401979446411133,
      "learning_rate": 0.000170981159774896,
      "loss": 0.4179,
      "step": 1440
    },
    {
      "epoch": 0.17326292054827033,
      "grad_norm": 1.349111795425415,
      "learning_rate": 0.0001704918032786885,
      "loss": 0.3974,
      "step": 1460
    },
    {
      "epoch": 0.17563638521331515,
      "grad_norm": 2.2389979362487793,
      "learning_rate": 0.00017000244678248106,
      "loss": 0.3974,
      "step": 1480
    },
    {
      "epoch": 0.17800984987835994,
      "grad_norm": 2.5390286445617676,
      "learning_rate": 0.00016951309028627356,
      "loss": 0.4003,
      "step": 1500
    },
    {
      "epoch": 0.18038331454340473,
      "grad_norm": 2.193808078765869,
      "learning_rate": 0.00016902373379006606,
      "loss": 0.3721,
      "step": 1520
    },
    {
      "epoch": 0.18275677920844954,
      "grad_norm": 1.0838831663131714,
      "learning_rate": 0.0001685343772938586,
      "loss": 0.3911,
      "step": 1540
    },
    {
      "epoch": 0.18513024387349433,
      "grad_norm": 1.458655595779419,
      "learning_rate": 0.00016804502079765108,
      "loss": 0.4281,
      "step": 1560
    },
    {
      "epoch": 0.18750370853853912,
      "grad_norm": 0.8876481056213379,
      "learning_rate": 0.0001675556643014436,
      "loss": 0.405,
      "step": 1580
    },
    {
      "epoch": 0.18987717320358394,
      "grad_norm": 1.2507541179656982,
      "learning_rate": 0.00016706630780523614,
      "loss": 0.3956,
      "step": 1600
    },
    {
      "epoch": 0.19225063786862873,
      "grad_norm": 1.2486152648925781,
      "learning_rate": 0.00016657695130902863,
      "loss": 0.3615,
      "step": 1620
    },
    {
      "epoch": 0.19462410253367354,
      "grad_norm": 1.1426784992218018,
      "learning_rate": 0.00016608759481282113,
      "loss": 0.3828,
      "step": 1640
    },
    {
      "epoch": 0.19699756719871833,
      "grad_norm": 1.2981197834014893,
      "learning_rate": 0.00016559823831661366,
      "loss": 0.3939,
      "step": 1660
    },
    {
      "epoch": 0.19937103186376312,
      "grad_norm": 1.9035186767578125,
      "learning_rate": 0.00016510888182040618,
      "loss": 0.3888,
      "step": 1680
    },
    {
      "epoch": 0.20174449652880794,
      "grad_norm": 1.4440691471099854,
      "learning_rate": 0.00016461952532419868,
      "loss": 0.4023,
      "step": 1700
    },
    {
      "epoch": 0.20411796119385273,
      "grad_norm": 1.2832502126693726,
      "learning_rate": 0.00016413016882799118,
      "loss": 0.3924,
      "step": 1720
    },
    {
      "epoch": 0.20649142585889751,
      "grad_norm": 1.7613017559051514,
      "learning_rate": 0.00016364081233178373,
      "loss": 0.3879,
      "step": 1740
    },
    {
      "epoch": 0.20886489052394233,
      "grad_norm": 1.5417588949203491,
      "learning_rate": 0.00016315145583557623,
      "loss": 0.3992,
      "step": 1760
    },
    {
      "epoch": 0.21123835518898712,
      "grad_norm": 1.4584345817565918,
      "learning_rate": 0.00016266209933936873,
      "loss": 0.3814,
      "step": 1780
    },
    {
      "epoch": 0.21361181985403194,
      "grad_norm": 1.274661898612976,
      "learning_rate": 0.00016217274284316123,
      "loss": 0.3712,
      "step": 1800
    },
    {
      "epoch": 0.21598528451907673,
      "grad_norm": 1.1549752950668335,
      "learning_rate": 0.00016168338634695378,
      "loss": 0.3733,
      "step": 1820
    },
    {
      "epoch": 0.21835874918412151,
      "grad_norm": 2.276437759399414,
      "learning_rate": 0.00016119402985074628,
      "loss": 0.4061,
      "step": 1840
    },
    {
      "epoch": 0.22073221384916633,
      "grad_norm": 1.4373914003372192,
      "learning_rate": 0.00016070467335453878,
      "loss": 0.3848,
      "step": 1860
    },
    {
      "epoch": 0.22310567851421112,
      "grad_norm": 1.739838719367981,
      "learning_rate": 0.0001602153168583313,
      "loss": 0.3706,
      "step": 1880
    },
    {
      "epoch": 0.2254791431792559,
      "grad_norm": 1.6130473613739014,
      "learning_rate": 0.00015972596036212383,
      "loss": 0.3738,
      "step": 1900
    },
    {
      "epoch": 0.22785260784430073,
      "grad_norm": 2.5463061332702637,
      "learning_rate": 0.00015923660386591633,
      "loss": 0.37,
      "step": 1920
    },
    {
      "epoch": 0.23022607250934551,
      "grad_norm": 1.969496250152588,
      "learning_rate": 0.00015874724736970885,
      "loss": 0.4035,
      "step": 1940
    },
    {
      "epoch": 0.2325995371743903,
      "grad_norm": 1.1532167196273804,
      "learning_rate": 0.00015825789087350135,
      "loss": 0.3902,
      "step": 1960
    },
    {
      "epoch": 0.23497300183943512,
      "grad_norm": 23.23250389099121,
      "learning_rate": 0.00015776853437729385,
      "loss": 0.3695,
      "step": 1980
    },
    {
      "epoch": 0.2373464665044799,
      "grad_norm": 2.1056292057037354,
      "learning_rate": 0.00015727917788108638,
      "loss": 0.4563,
      "step": 2000
    },
    {
      "epoch": 0.23971993116952472,
      "grad_norm": 1.1265405416488647,
      "learning_rate": 0.0001567898213848789,
      "loss": 0.3732,
      "step": 2020
    },
    {
      "epoch": 0.2420933958345695,
      "grad_norm": 1.9109147787094116,
      "learning_rate": 0.0001563004648886714,
      "loss": 0.362,
      "step": 2040
    },
    {
      "epoch": 0.2444668604996143,
      "grad_norm": 2.5601611137390137,
      "learning_rate": 0.0001558111083924639,
      "loss": 0.3859,
      "step": 2060
    },
    {
      "epoch": 0.24684032516465912,
      "grad_norm": 1.4311975240707397,
      "learning_rate": 0.00015532175189625645,
      "loss": 0.3929,
      "step": 2080
    },
    {
      "epoch": 0.2492137898297039,
      "grad_norm": 3.8879075050354004,
      "learning_rate": 0.00015483239540004895,
      "loss": 0.4119,
      "step": 2100
    },
    {
      "epoch": 0.2515872544947487,
      "grad_norm": 1.2271177768707275,
      "learning_rate": 0.00015434303890384145,
      "loss": 0.3861,
      "step": 2120
    },
    {
      "epoch": 0.2539607191597935,
      "grad_norm": 2.562037706375122,
      "learning_rate": 0.00015385368240763395,
      "loss": 0.4015,
      "step": 2140
    },
    {
      "epoch": 0.2563341838248383,
      "grad_norm": 1.3863781690597534,
      "learning_rate": 0.0001533643259114265,
      "loss": 0.371,
      "step": 2160
    },
    {
      "epoch": 0.2587076484898831,
      "grad_norm": 1.3612499237060547,
      "learning_rate": 0.000152874969415219,
      "loss": 0.3791,
      "step": 2180
    },
    {
      "epoch": 0.2610811131549279,
      "grad_norm": 2.164926528930664,
      "learning_rate": 0.0001523856129190115,
      "loss": 0.3744,
      "step": 2200
    },
    {
      "epoch": 0.2634545778199727,
      "grad_norm": 1.8500858545303345,
      "learning_rate": 0.00015189625642280402,
      "loss": 0.3806,
      "step": 2220
    },
    {
      "epoch": 0.2658280424850175,
      "grad_norm": 1.6163365840911865,
      "learning_rate": 0.00015140689992659655,
      "loss": 0.3744,
      "step": 2240
    },
    {
      "epoch": 0.2682015071500623,
      "grad_norm": 1.0475815534591675,
      "learning_rate": 0.00015091754343038905,
      "loss": 0.3923,
      "step": 2260
    },
    {
      "epoch": 0.2705749718151071,
      "grad_norm": 2.6741323471069336,
      "learning_rate": 0.00015042818693418154,
      "loss": 0.373,
      "step": 2280
    },
    {
      "epoch": 0.2729484364801519,
      "grad_norm": 1.9894195795059204,
      "learning_rate": 0.00014993883043797407,
      "loss": 0.3581,
      "step": 2300
    },
    {
      "epoch": 0.2753219011451967,
      "grad_norm": 1.2157033681869507,
      "learning_rate": 0.0001494494739417666,
      "loss": 0.3873,
      "step": 2320
    },
    {
      "epoch": 0.2776953658102415,
      "grad_norm": 2.523430109024048,
      "learning_rate": 0.0001489601174455591,
      "loss": 0.3876,
      "step": 2340
    },
    {
      "epoch": 0.2800688304752863,
      "grad_norm": 2.0556445121765137,
      "learning_rate": 0.00014847076094935162,
      "loss": 0.3638,
      "step": 2360
    },
    {
      "epoch": 0.2824422951403311,
      "grad_norm": 0.7185204029083252,
      "learning_rate": 0.00014798140445314412,
      "loss": 0.3844,
      "step": 2380
    },
    {
      "epoch": 0.2848157598053759,
      "grad_norm": 1.8392207622528076,
      "learning_rate": 0.00014749204795693664,
      "loss": 0.3643,
      "step": 2400
    },
    {
      "epoch": 0.2871892244704207,
      "grad_norm": 2.34373140335083,
      "learning_rate": 0.00014700269146072917,
      "loss": 0.3982,
      "step": 2420
    },
    {
      "epoch": 0.2895626891354655,
      "grad_norm": 2.3018035888671875,
      "learning_rate": 0.00014651333496452167,
      "loss": 0.4034,
      "step": 2440
    },
    {
      "epoch": 0.2919361538005103,
      "grad_norm": 2.122706890106201,
      "learning_rate": 0.00014602397846831417,
      "loss": 0.456,
      "step": 2460
    },
    {
      "epoch": 0.2943096184655551,
      "grad_norm": 3.3078296184539795,
      "learning_rate": 0.00014553462197210666,
      "loss": 0.4208,
      "step": 2480
    },
    {
      "epoch": 0.2966830831305999,
      "grad_norm": 204.794189453125,
      "learning_rate": 0.00014504526547589922,
      "loss": 0.4106,
      "step": 2500
    },
    {
      "epoch": 0.29905654779564467,
      "grad_norm": 0.9853575229644775,
      "learning_rate": 0.00014455590897969172,
      "loss": 0.3362,
      "step": 2520
    },
    {
      "epoch": 0.3014300124606895,
      "grad_norm": 1.9290248155593872,
      "learning_rate": 0.0001440665524834842,
      "loss": 0.4015,
      "step": 2540
    },
    {
      "epoch": 0.3038034771257343,
      "grad_norm": 1.8448785543441772,
      "learning_rate": 0.00014357719598727674,
      "loss": 0.3609,
      "step": 2560
    },
    {
      "epoch": 0.3061769417907791,
      "grad_norm": 2.3156535625457764,
      "learning_rate": 0.00014308783949106926,
      "loss": 0.4004,
      "step": 2580
    },
    {
      "epoch": 0.3085504064558239,
      "grad_norm": 1.7891960144042969,
      "learning_rate": 0.00014259848299486176,
      "loss": 0.385,
      "step": 2600
    },
    {
      "epoch": 0.31092387112086867,
      "grad_norm": 9.332310676574707,
      "learning_rate": 0.00014210912649865426,
      "loss": 0.3887,
      "step": 2620
    },
    {
      "epoch": 0.3132973357859135,
      "grad_norm": 2.8031811714172363,
      "learning_rate": 0.0001416197700024468,
      "loss": 0.7171,
      "step": 2640
    },
    {
      "epoch": 0.3156708004509583,
      "grad_norm": 1.9550771713256836,
      "learning_rate": 0.0001411304135062393,
      "loss": 0.4746,
      "step": 2660
    },
    {
      "epoch": 0.3180442651160031,
      "grad_norm": 3.6590027809143066,
      "learning_rate": 0.0001406410570100318,
      "loss": 0.4187,
      "step": 2680
    },
    {
      "epoch": 0.3204177297810479,
      "grad_norm": 3.734830856323242,
      "learning_rate": 0.00014015170051382434,
      "loss": 0.4061,
      "step": 2700
    },
    {
      "epoch": 0.32279119444609267,
      "grad_norm": 3.1509487628936768,
      "learning_rate": 0.00013966234401761683,
      "loss": 0.3867,
      "step": 2720
    },
    {
      "epoch": 0.3251646591111375,
      "grad_norm": 1.944064974784851,
      "learning_rate": 0.00013917298752140936,
      "loss": 0.3636,
      "step": 2740
    },
    {
      "epoch": 0.3275381237761823,
      "grad_norm": 3.6596593856811523,
      "learning_rate": 0.00013868363102520189,
      "loss": 0.3891,
      "step": 2760
    },
    {
      "epoch": 0.3299115884412271,
      "grad_norm": 2.4684841632843018,
      "learning_rate": 0.00013819427452899438,
      "loss": 0.4214,
      "step": 2780
    },
    {
      "epoch": 0.3322850531062719,
      "grad_norm": 1.5733566284179688,
      "learning_rate": 0.00013770491803278688,
      "loss": 0.4099,
      "step": 2800
    },
    {
      "epoch": 0.33465851777131667,
      "grad_norm": 2.87282657623291,
      "learning_rate": 0.0001372155615365794,
      "loss": 0.4001,
      "step": 2820
    },
    {
      "epoch": 0.33703198243636145,
      "grad_norm": 1.3817462921142578,
      "learning_rate": 0.00013672620504037193,
      "loss": 0.3718,
      "step": 2840
    },
    {
      "epoch": 0.3394054471014063,
      "grad_norm": 1.9604424238204956,
      "learning_rate": 0.00013623684854416443,
      "loss": 0.4133,
      "step": 2860
    },
    {
      "epoch": 0.3417789117664511,
      "grad_norm": 1.0304818153381348,
      "learning_rate": 0.00013574749204795693,
      "loss": 0.3947,
      "step": 2880
    },
    {
      "epoch": 0.3441523764314959,
      "grad_norm": 1.89744234085083,
      "learning_rate": 0.00013525813555174946,
      "loss": 0.3869,
      "step": 2900
    },
    {
      "epoch": 0.34652584109654067,
      "grad_norm": 1.7879353761672974,
      "learning_rate": 0.00013476877905554198,
      "loss": 0.3869,
      "step": 2920
    },
    {
      "epoch": 0.34889930576158545,
      "grad_norm": 3.3424434661865234,
      "learning_rate": 0.00013427942255933448,
      "loss": 0.3857,
      "step": 2940
    },
    {
      "epoch": 0.3512727704266303,
      "grad_norm": 1.1346714496612549,
      "learning_rate": 0.00013379006606312698,
      "loss": 0.3917,
      "step": 2960
    },
    {
      "epoch": 0.3536462350916751,
      "grad_norm": 1.4750539064407349,
      "learning_rate": 0.0001333007095669195,
      "loss": 0.3819,
      "step": 2980
    },
    {
      "epoch": 0.3560196997567199,
      "grad_norm": 12.61292839050293,
      "learning_rate": 0.00013281135307071203,
      "loss": 0.3795,
      "step": 3000
    },
    {
      "epoch": 0.35839316442176467,
      "grad_norm": 2.211517810821533,
      "learning_rate": 0.00013232199657450453,
      "loss": 0.4019,
      "step": 3020
    },
    {
      "epoch": 0.36076662908680945,
      "grad_norm": 2.35832142829895,
      "learning_rate": 0.00013183264007829705,
      "loss": 0.3871,
      "step": 3040
    },
    {
      "epoch": 0.36314009375185424,
      "grad_norm": 8.54818344116211,
      "learning_rate": 0.00013134328358208955,
      "loss": 0.639,
      "step": 3060
    },
    {
      "epoch": 0.3655135584168991,
      "grad_norm": 1.245188593864441,
      "learning_rate": 0.00013085392708588208,
      "loss": 0.3548,
      "step": 3080
    },
    {
      "epoch": 0.3678870230819439,
      "grad_norm": 1.3198671340942383,
      "learning_rate": 0.0001303645705896746,
      "loss": 0.3668,
      "step": 3100
    },
    {
      "epoch": 0.37026048774698866,
      "grad_norm": 1.478326678276062,
      "learning_rate": 0.0001298752140934671,
      "loss": 0.374,
      "step": 3120
    },
    {
      "epoch": 0.37263395241203345,
      "grad_norm": 2.6946959495544434,
      "learning_rate": 0.0001293858575972596,
      "loss": 0.3646,
      "step": 3140
    },
    {
      "epoch": 0.37500741707707824,
      "grad_norm": 2.523125410079956,
      "learning_rate": 0.00012889650110105213,
      "loss": 0.368,
      "step": 3160
    },
    {
      "epoch": 0.3773808817421231,
      "grad_norm": 3.3064894676208496,
      "learning_rate": 0.00012840714460484465,
      "loss": 0.3897,
      "step": 3180
    },
    {
      "epoch": 0.3797543464071679,
      "grad_norm": 1.6155154705047607,
      "learning_rate": 0.00012791778810863715,
      "loss": 0.3681,
      "step": 3200
    },
    {
      "epoch": 0.38212781107221266,
      "grad_norm": 1.6991246938705444,
      "learning_rate": 0.00012742843161242965,
      "loss": 0.3561,
      "step": 3220
    },
    {
      "epoch": 0.38450127573725745,
      "grad_norm": 1.5448497533798218,
      "learning_rate": 0.00012693907511622217,
      "loss": 0.3934,
      "step": 3240
    },
    {
      "epoch": 0.38687474040230224,
      "grad_norm": 3.22839093208313,
      "learning_rate": 0.0001264497186200147,
      "loss": 0.3913,
      "step": 3260
    },
    {
      "epoch": 0.3892482050673471,
      "grad_norm": 2.2244668006896973,
      "learning_rate": 0.0001259603621238072,
      "loss": 0.3904,
      "step": 3280
    },
    {
      "epoch": 0.3916216697323919,
      "grad_norm": 1.205551266670227,
      "learning_rate": 0.0001254710056275997,
      "loss": 0.3695,
      "step": 3300
    },
    {
      "epoch": 0.39399513439743666,
      "grad_norm": 1.655707836151123,
      "learning_rate": 0.00012498164913139222,
      "loss": 0.3811,
      "step": 3320
    },
    {
      "epoch": 0.39636859906248145,
      "grad_norm": 2.7230887413024902,
      "learning_rate": 0.00012449229263518475,
      "loss": 0.3705,
      "step": 3340
    },
    {
      "epoch": 0.39874206372752624,
      "grad_norm": 2.947999954223633,
      "learning_rate": 0.00012400293613897725,
      "loss": 0.3661,
      "step": 3360
    },
    {
      "epoch": 0.40111552839257103,
      "grad_norm": 1.8846107721328735,
      "learning_rate": 0.00012351357964276977,
      "loss": 0.3764,
      "step": 3380
    },
    {
      "epoch": 0.4034889930576159,
      "grad_norm": 1.4596452713012695,
      "learning_rate": 0.00012302422314656227,
      "loss": 0.3779,
      "step": 3400
    },
    {
      "epoch": 0.40586245772266066,
      "grad_norm": 2.2762773036956787,
      "learning_rate": 0.0001225348666503548,
      "loss": 0.3619,
      "step": 3420
    },
    {
      "epoch": 0.40823592238770545,
      "grad_norm": 2.1612789630889893,
      "learning_rate": 0.0001220455101541473,
      "loss": 0.397,
      "step": 3440
    },
    {
      "epoch": 0.41060938705275024,
      "grad_norm": 1.3056119680404663,
      "learning_rate": 0.00012155615365793982,
      "loss": 0.3944,
      "step": 3460
    },
    {
      "epoch": 0.41298285171779503,
      "grad_norm": 1.695682168006897,
      "learning_rate": 0.00012106679716173233,
      "loss": 0.3672,
      "step": 3480
    },
    {
      "epoch": 0.4153563163828399,
      "grad_norm": 2.3700876235961914,
      "learning_rate": 0.00012057744066552483,
      "loss": 0.3865,
      "step": 3500
    },
    {
      "epoch": 0.41772978104788466,
      "grad_norm": 1.7023401260375977,
      "learning_rate": 0.00012008808416931736,
      "loss": 0.3681,
      "step": 3520
    },
    {
      "epoch": 0.42010324571292945,
      "grad_norm": 1.4773986339569092,
      "learning_rate": 0.00011959872767310987,
      "loss": 0.3773,
      "step": 3540
    },
    {
      "epoch": 0.42247671037797424,
      "grad_norm": 1.983884334564209,
      "learning_rate": 0.00011910937117690238,
      "loss": 0.3926,
      "step": 3560
    },
    {
      "epoch": 0.42485017504301903,
      "grad_norm": 2.1872928142547607,
      "learning_rate": 0.0001186200146806949,
      "loss": 0.3419,
      "step": 3580
    },
    {
      "epoch": 0.4272236397080639,
      "grad_norm": 1.8092525005340576,
      "learning_rate": 0.0001181306581844874,
      "loss": 0.3804,
      "step": 3600
    },
    {
      "epoch": 0.42959710437310866,
      "grad_norm": 1.2800947427749634,
      "learning_rate": 0.00011764130168827992,
      "loss": 0.3704,
      "step": 3620
    },
    {
      "epoch": 0.43197056903815345,
      "grad_norm": 1.7970985174179077,
      "learning_rate": 0.00011715194519207243,
      "loss": 0.3693,
      "step": 3640
    },
    {
      "epoch": 0.43434403370319824,
      "grad_norm": 1.917539358139038,
      "learning_rate": 0.00011666258869586495,
      "loss": 0.3835,
      "step": 3660
    },
    {
      "epoch": 0.43671749836824303,
      "grad_norm": 1.8106557130813599,
      "learning_rate": 0.00011617323219965745,
      "loss": 0.3772,
      "step": 3680
    },
    {
      "epoch": 0.4390909630332878,
      "grad_norm": 6.917564868927002,
      "learning_rate": 0.00011568387570344996,
      "loss": 0.4785,
      "step": 3700
    },
    {
      "epoch": 0.44146442769833266,
      "grad_norm": 2.4841582775115967,
      "learning_rate": 0.00011519451920724249,
      "loss": 0.3669,
      "step": 3720
    },
    {
      "epoch": 0.44383789236337745,
      "grad_norm": 1.4030264616012573,
      "learning_rate": 0.000114705162711035,
      "loss": 0.3836,
      "step": 3740
    },
    {
      "epoch": 0.44621135702842224,
      "grad_norm": 20.186141967773438,
      "learning_rate": 0.0001142158062148275,
      "loss": 0.3957,
      "step": 3760
    },
    {
      "epoch": 0.44858482169346703,
      "grad_norm": 4.004581928253174,
      "learning_rate": 0.00011372644971862001,
      "loss": 0.4399,
      "step": 3780
    },
    {
      "epoch": 0.4509582863585118,
      "grad_norm": 2.194579839706421,
      "learning_rate": 0.00011323709322241254,
      "loss": 0.3693,
      "step": 3800
    },
    {
      "epoch": 0.45333175102355666,
      "grad_norm": 2.722611427307129,
      "learning_rate": 0.00011274773672620505,
      "loss": 0.3891,
      "step": 3820
    },
    {
      "epoch": 0.45570521568860145,
      "grad_norm": 1.9015978574752808,
      "learning_rate": 0.00011225838022999755,
      "loss": 0.3821,
      "step": 3840
    },
    {
      "epoch": 0.45807868035364624,
      "grad_norm": 2.0209922790527344,
      "learning_rate": 0.00011176902373379007,
      "loss": 0.3739,
      "step": 3860
    },
    {
      "epoch": 0.46045214501869103,
      "grad_norm": 1.9847983121871948,
      "learning_rate": 0.00011127966723758258,
      "loss": 0.3655,
      "step": 3880
    },
    {
      "epoch": 0.4628256096837358,
      "grad_norm": 1.651368260383606,
      "learning_rate": 0.0001107903107413751,
      "loss": 0.3617,
      "step": 3900
    },
    {
      "epoch": 0.4651990743487806,
      "grad_norm": 2.5829432010650635,
      "learning_rate": 0.00011030095424516762,
      "loss": 0.374,
      "step": 3920
    },
    {
      "epoch": 0.46757253901382545,
      "grad_norm": 1.6545594930648804,
      "learning_rate": 0.00010981159774896012,
      "loss": 0.3733,
      "step": 3940
    },
    {
      "epoch": 0.46994600367887024,
      "grad_norm": 1.2478442192077637,
      "learning_rate": 0.00010932224125275263,
      "loss": 0.3705,
      "step": 3960
    },
    {
      "epoch": 0.472319468343915,
      "grad_norm": 1.313157320022583,
      "learning_rate": 0.00010883288475654514,
      "loss": 0.3767,
      "step": 3980
    },
    {
      "epoch": 0.4746929330089598,
      "grad_norm": 3.253657102584839,
      "learning_rate": 0.00010834352826033767,
      "loss": 0.3689,
      "step": 4000
    },
    {
      "epoch": 0.4770663976740046,
      "grad_norm": 1.1668720245361328,
      "learning_rate": 0.00010785417176413017,
      "loss": 0.3549,
      "step": 4020
    },
    {
      "epoch": 0.47943986233904945,
      "grad_norm": 2.6294593811035156,
      "learning_rate": 0.00010736481526792268,
      "loss": 0.3515,
      "step": 4040
    },
    {
      "epoch": 0.48181332700409424,
      "grad_norm": 1.420029640197754,
      "learning_rate": 0.0001068754587717152,
      "loss": 0.355,
      "step": 4060
    },
    {
      "epoch": 0.484186791669139,
      "grad_norm": 3.0312163829803467,
      "learning_rate": 0.00010638610227550772,
      "loss": 0.3884,
      "step": 4080
    },
    {
      "epoch": 0.4865602563341838,
      "grad_norm": 1.292585015296936,
      "learning_rate": 0.00010589674577930022,
      "loss": 0.3743,
      "step": 4100
    },
    {
      "epoch": 0.4889337209992286,
      "grad_norm": 1.2691729068756104,
      "learning_rate": 0.00010540738928309273,
      "loss": 0.361,
      "step": 4120
    },
    {
      "epoch": 0.49130718566427345,
      "grad_norm": 1.1584625244140625,
      "learning_rate": 0.00010491803278688525,
      "loss": 0.3712,
      "step": 4140
    },
    {
      "epoch": 0.49368065032931824,
      "grad_norm": 2.3669166564941406,
      "learning_rate": 0.00010442867629067777,
      "loss": 0.3685,
      "step": 4160
    },
    {
      "epoch": 0.496054114994363,
      "grad_norm": 0.7582140564918518,
      "learning_rate": 0.00010393931979447026,
      "loss": 0.3759,
      "step": 4180
    },
    {
      "epoch": 0.4984275796594078,
      "grad_norm": 1.7281068563461304,
      "learning_rate": 0.0001034499632982628,
      "loss": 0.3715,
      "step": 4200
    },
    {
      "epoch": 0.5008010443244526,
      "grad_norm": 2.0083084106445312,
      "learning_rate": 0.0001029606068020553,
      "loss": 0.353,
      "step": 4220
    },
    {
      "epoch": 0.5031745089894974,
      "grad_norm": 2.3803181648254395,
      "learning_rate": 0.00010247125030584781,
      "loss": 0.3698,
      "step": 4240
    },
    {
      "epoch": 0.5055479736545422,
      "grad_norm": 4.1114935874938965,
      "learning_rate": 0.00010198189380964034,
      "loss": 0.3754,
      "step": 4260
    },
    {
      "epoch": 0.507921438319587,
      "grad_norm": 2.068995714187622,
      "learning_rate": 0.00010149253731343284,
      "loss": 0.335,
      "step": 4280
    },
    {
      "epoch": 0.5102949029846319,
      "grad_norm": 1.597364068031311,
      "learning_rate": 0.00010100318081722535,
      "loss": 0.3429,
      "step": 4300
    },
    {
      "epoch": 0.5126683676496766,
      "grad_norm": 2.208829641342163,
      "learning_rate": 0.00010051382432101786,
      "loss": 0.3847,
      "step": 4320
    },
    {
      "epoch": 0.5150418323147214,
      "grad_norm": 1.7073098421096802,
      "learning_rate": 0.00010002446782481039,
      "loss": 0.3851,
      "step": 4340
    },
    {
      "epoch": 0.5174152969797662,
      "grad_norm": 1.796923041343689,
      "learning_rate": 9.953511132860289e-05,
      "loss": 0.3753,
      "step": 4360
    },
    {
      "epoch": 0.519788761644811,
      "grad_norm": 1.4333096742630005,
      "learning_rate": 9.904575483239541e-05,
      "loss": 0.3691,
      "step": 4380
    },
    {
      "epoch": 0.5221622263098558,
      "grad_norm": 1.9864917993545532,
      "learning_rate": 9.855639833618791e-05,
      "loss": 0.3729,
      "step": 4400
    },
    {
      "epoch": 0.5245356909749006,
      "grad_norm": 1.7580136060714722,
      "learning_rate": 9.806704183998044e-05,
      "loss": 0.3791,
      "step": 4420
    },
    {
      "epoch": 0.5269091556399454,
      "grad_norm": 1.6604807376861572,
      "learning_rate": 9.757768534377293e-05,
      "loss": 0.3917,
      "step": 4440
    },
    {
      "epoch": 0.5292826203049902,
      "grad_norm": 3.3625121116638184,
      "learning_rate": 9.708832884756546e-05,
      "loss": 0.3904,
      "step": 4460
    },
    {
      "epoch": 0.531656084970035,
      "grad_norm": 1.9359862804412842,
      "learning_rate": 9.659897235135796e-05,
      "loss": 0.3679,
      "step": 4480
    },
    {
      "epoch": 0.5340295496350798,
      "grad_norm": 1.9828983545303345,
      "learning_rate": 9.610961585515048e-05,
      "loss": 0.3825,
      "step": 4500
    },
    {
      "epoch": 0.5364030143001246,
      "grad_norm": 1.7152009010314941,
      "learning_rate": 9.5620259358943e-05,
      "loss": 0.3611,
      "step": 4520
    },
    {
      "epoch": 0.5387764789651694,
      "grad_norm": 2.014103412628174,
      "learning_rate": 9.513090286273551e-05,
      "loss": 0.3753,
      "step": 4540
    },
    {
      "epoch": 0.5411499436302142,
      "grad_norm": 3.035949468612671,
      "learning_rate": 9.464154636652802e-05,
      "loss": 0.3577,
      "step": 4560
    },
    {
      "epoch": 0.543523408295259,
      "grad_norm": 1.8329880237579346,
      "learning_rate": 9.415218987032053e-05,
      "loss": 0.3605,
      "step": 4580
    },
    {
      "epoch": 0.5458968729603038,
      "grad_norm": 1.6098233461380005,
      "learning_rate": 9.366283337411304e-05,
      "loss": 0.3616,
      "step": 4600
    },
    {
      "epoch": 0.5482703376253486,
      "grad_norm": 4.185550212860107,
      "learning_rate": 9.317347687790557e-05,
      "loss": 0.3756,
      "step": 4620
    },
    {
      "epoch": 0.5506438022903934,
      "grad_norm": 2.2307426929473877,
      "learning_rate": 9.268412038169807e-05,
      "loss": 0.3704,
      "step": 4640
    },
    {
      "epoch": 0.5530172669554382,
      "grad_norm": 1.4223692417144775,
      "learning_rate": 9.21947638854906e-05,
      "loss": 0.37,
      "step": 4660
    },
    {
      "epoch": 0.555390731620483,
      "grad_norm": 2.6643881797790527,
      "learning_rate": 9.170540738928309e-05,
      "loss": 0.3654,
      "step": 4680
    },
    {
      "epoch": 0.5577641962855278,
      "grad_norm": 1.724628210067749,
      "learning_rate": 9.121605089307562e-05,
      "loss": 0.347,
      "step": 4700
    },
    {
      "epoch": 0.5601376609505726,
      "grad_norm": 1.5103131532669067,
      "learning_rate": 9.072669439686813e-05,
      "loss": 0.3691,
      "step": 4720
    },
    {
      "epoch": 0.5625111256156174,
      "grad_norm": 2.040606737136841,
      "learning_rate": 9.023733790066063e-05,
      "loss": 0.3491,
      "step": 4740
    },
    {
      "epoch": 0.5648845902806622,
      "grad_norm": 1.935078501701355,
      "learning_rate": 8.974798140445315e-05,
      "loss": 0.3489,
      "step": 4760
    },
    {
      "epoch": 0.567258054945707,
      "grad_norm": 1.6672091484069824,
      "learning_rate": 8.925862490824565e-05,
      "loss": 0.3792,
      "step": 4780
    },
    {
      "epoch": 0.5696315196107518,
      "grad_norm": 1.8741642236709595,
      "learning_rate": 8.876926841203818e-05,
      "loss": 0.3753,
      "step": 4800
    },
    {
      "epoch": 0.5720049842757966,
      "grad_norm": 2.4180009365081787,
      "learning_rate": 8.827991191583068e-05,
      "loss": 0.3608,
      "step": 4820
    },
    {
      "epoch": 0.5743784489408414,
      "grad_norm": 2.620514392852783,
      "learning_rate": 8.77905554196232e-05,
      "loss": 0.3877,
      "step": 4840
    },
    {
      "epoch": 0.5767519136058862,
      "grad_norm": 1.6868575811386108,
      "learning_rate": 8.730119892341571e-05,
      "loss": 0.3331,
      "step": 4860
    },
    {
      "epoch": 0.579125378270931,
      "grad_norm": 1.4588055610656738,
      "learning_rate": 8.681184242720823e-05,
      "loss": 0.3711,
      "step": 4880
    },
    {
      "epoch": 0.5814988429359758,
      "grad_norm": 1.654272198677063,
      "learning_rate": 8.632248593100074e-05,
      "loss": 0.3913,
      "step": 4900
    },
    {
      "epoch": 0.5838723076010206,
      "grad_norm": 1.3486685752868652,
      "learning_rate": 8.583312943479325e-05,
      "loss": 0.366,
      "step": 4920
    },
    {
      "epoch": 0.5862457722660653,
      "grad_norm": 1.5217593908309937,
      "learning_rate": 8.534377293858576e-05,
      "loss": 0.3513,
      "step": 4940
    },
    {
      "epoch": 0.5886192369311102,
      "grad_norm": 1.7261992692947388,
      "learning_rate": 8.485441644237829e-05,
      "loss": 0.3442,
      "step": 4960
    },
    {
      "epoch": 0.590992701596155,
      "grad_norm": 52.888240814208984,
      "learning_rate": 8.436505994617079e-05,
      "loss": 0.366,
      "step": 4980
    },
    {
      "epoch": 0.5933661662611998,
      "grad_norm": 2.364020824432373,
      "learning_rate": 8.387570344996331e-05,
      "loss": 0.3732,
      "step": 5000
    },
    {
      "epoch": 0.5957396309262446,
      "grad_norm": 1.7898871898651123,
      "learning_rate": 8.338634695375581e-05,
      "loss": 0.3726,
      "step": 5020
    },
    {
      "epoch": 0.5981130955912893,
      "grad_norm": 1.9258311986923218,
      "learning_rate": 8.289699045754833e-05,
      "loss": 0.3646,
      "step": 5040
    },
    {
      "epoch": 0.6004865602563342,
      "grad_norm": 1.1341828107833862,
      "learning_rate": 8.240763396134083e-05,
      "loss": 0.3929,
      "step": 5060
    },
    {
      "epoch": 0.602860024921379,
      "grad_norm": 1.54243004322052,
      "learning_rate": 8.191827746513336e-05,
      "loss": 0.3614,
      "step": 5080
    },
    {
      "epoch": 0.6052334895864238,
      "grad_norm": 2.4663307666778564,
      "learning_rate": 8.142892096892587e-05,
      "loss": 0.3892,
      "step": 5100
    },
    {
      "epoch": 0.6076069542514686,
      "grad_norm": 2.0522608757019043,
      "learning_rate": 8.093956447271838e-05,
      "loss": 0.3975,
      "step": 5120
    },
    {
      "epoch": 0.6099804189165133,
      "grad_norm": 2.3828043937683105,
      "learning_rate": 8.04502079765109e-05,
      "loss": 0.3752,
      "step": 5140
    },
    {
      "epoch": 0.6123538835815582,
      "grad_norm": 3.431532859802246,
      "learning_rate": 7.996085148030341e-05,
      "loss": 0.4037,
      "step": 5160
    },
    {
      "epoch": 0.614727348246603,
      "grad_norm": 1.9761924743652344,
      "learning_rate": 7.947149498409592e-05,
      "loss": 0.3703,
      "step": 5180
    },
    {
      "epoch": 0.6171008129116478,
      "grad_norm": 2.8015425205230713,
      "learning_rate": 7.898213848788843e-05,
      "loss": 0.3672,
      "step": 5200
    },
    {
      "epoch": 0.6194742775766926,
      "grad_norm": 1.47091805934906,
      "learning_rate": 7.849278199168094e-05,
      "loss": 0.3798,
      "step": 5220
    },
    {
      "epoch": 0.6218477422417373,
      "grad_norm": 1.898072361946106,
      "learning_rate": 7.800342549547345e-05,
      "loss": 0.3839,
      "step": 5240
    },
    {
      "epoch": 0.6242212069067822,
      "grad_norm": 1.5792988538742065,
      "learning_rate": 7.751406899926597e-05,
      "loss": 0.3545,
      "step": 5260
    },
    {
      "epoch": 0.626594671571827,
      "grad_norm": 2.2192764282226562,
      "learning_rate": 7.702471250305848e-05,
      "loss": 0.3856,
      "step": 5280
    },
    {
      "epoch": 0.6289681362368718,
      "grad_norm": 0.7627376914024353,
      "learning_rate": 7.6535356006851e-05,
      "loss": 0.3642,
      "step": 5300
    },
    {
      "epoch": 0.6313416009019166,
      "grad_norm": 4.046471118927002,
      "learning_rate": 7.60459995106435e-05,
      "loss": 0.3666,
      "step": 5320
    },
    {
      "epoch": 0.6337150655669613,
      "grad_norm": 1.7396111488342285,
      "learning_rate": 7.555664301443603e-05,
      "loss": 0.3561,
      "step": 5340
    },
    {
      "epoch": 0.6360885302320062,
      "grad_norm": 1.1596630811691284,
      "learning_rate": 7.506728651822853e-05,
      "loss": 0.381,
      "step": 5360
    },
    {
      "epoch": 0.638461994897051,
      "grad_norm": 1.7776880264282227,
      "learning_rate": 7.457793002202105e-05,
      "loss": 0.3517,
      "step": 5380
    },
    {
      "epoch": 0.6408354595620958,
      "grad_norm": 3.6066153049468994,
      "learning_rate": 7.408857352581355e-05,
      "loss": 0.3688,
      "step": 5400
    },
    {
      "epoch": 0.6432089242271406,
      "grad_norm": 2.4223973751068115,
      "learning_rate": 7.359921702960608e-05,
      "loss": 0.3785,
      "step": 5420
    },
    {
      "epoch": 0.6455823888921853,
      "grad_norm": 2.286912441253662,
      "learning_rate": 7.310986053339859e-05,
      "loss": 0.3664,
      "step": 5440
    },
    {
      "epoch": 0.6479558535572302,
      "grad_norm": 3.0928664207458496,
      "learning_rate": 7.26205040371911e-05,
      "loss": 0.3715,
      "step": 5460
    },
    {
      "epoch": 0.650329318222275,
      "grad_norm": 1.1870402097702026,
      "learning_rate": 7.213114754098361e-05,
      "loss": 0.3726,
      "step": 5480
    },
    {
      "epoch": 0.6527027828873198,
      "grad_norm": 2.2675700187683105,
      "learning_rate": 7.164179104477612e-05,
      "loss": 0.3674,
      "step": 5500
    },
    {
      "epoch": 0.6550762475523646,
      "grad_norm": 1.4098347425460815,
      "learning_rate": 7.115243454856864e-05,
      "loss": 0.34,
      "step": 5520
    },
    {
      "epoch": 0.6574497122174093,
      "grad_norm": 2.6076536178588867,
      "learning_rate": 7.066307805236115e-05,
      "loss": 0.3646,
      "step": 5540
    },
    {
      "epoch": 0.6598231768824542,
      "grad_norm": 2.0717947483062744,
      "learning_rate": 7.017372155615366e-05,
      "loss": 0.3512,
      "step": 5560
    },
    {
      "epoch": 0.6621966415474989,
      "grad_norm": 2.94180965423584,
      "learning_rate": 6.968436505994617e-05,
      "loss": 0.3383,
      "step": 5580
    },
    {
      "epoch": 0.6645701062125438,
      "grad_norm": 1.6055091619491577,
      "learning_rate": 6.919500856373868e-05,
      "loss": 0.3542,
      "step": 5600
    },
    {
      "epoch": 0.6669435708775886,
      "grad_norm": 3.7483103275299072,
      "learning_rate": 6.87056520675312e-05,
      "loss": 0.3695,
      "step": 5620
    },
    {
      "epoch": 0.6693170355426333,
      "grad_norm": 4.2185444831848145,
      "learning_rate": 6.821629557132371e-05,
      "loss": 0.3665,
      "step": 5640
    },
    {
      "epoch": 0.6716905002076782,
      "grad_norm": 1.6032052040100098,
      "learning_rate": 6.772693907511622e-05,
      "loss": 0.348,
      "step": 5660
    },
    {
      "epoch": 0.6740639648727229,
      "grad_norm": 3.2844979763031006,
      "learning_rate": 6.723758257890875e-05,
      "loss": 0.375,
      "step": 5680
    },
    {
      "epoch": 0.6764374295377678,
      "grad_norm": 1.5148241519927979,
      "learning_rate": 6.674822608270124e-05,
      "loss": 0.3754,
      "step": 5700
    },
    {
      "epoch": 0.6788108942028126,
      "grad_norm": 2.5436980724334717,
      "learning_rate": 6.625886958649377e-05,
      "loss": 0.3665,
      "step": 5720
    },
    {
      "epoch": 0.6811843588678573,
      "grad_norm": 1.597674012184143,
      "learning_rate": 6.576951309028627e-05,
      "loss": 0.3398,
      "step": 5740
    },
    {
      "epoch": 0.6835578235329022,
      "grad_norm": 2.0251364707946777,
      "learning_rate": 6.52801565940788e-05,
      "loss": 0.3666,
      "step": 5760
    },
    {
      "epoch": 0.6859312881979469,
      "grad_norm": 1.3586581945419312,
      "learning_rate": 6.47908000978713e-05,
      "loss": 0.36,
      "step": 5780
    },
    {
      "epoch": 0.6883047528629918,
      "grad_norm": 0.9199285507202148,
      "learning_rate": 6.430144360166382e-05,
      "loss": 0.3511,
      "step": 5800
    },
    {
      "epoch": 0.6906782175280366,
      "grad_norm": 2.5790820121765137,
      "learning_rate": 6.381208710545633e-05,
      "loss": 0.3481,
      "step": 5820
    },
    {
      "epoch": 0.6930516821930813,
      "grad_norm": 2.834559440612793,
      "learning_rate": 6.332273060924884e-05,
      "loss": 0.3378,
      "step": 5840
    },
    {
      "epoch": 0.6954251468581262,
      "grad_norm": 1.4175961017608643,
      "learning_rate": 6.283337411304135e-05,
      "loss": 0.3759,
      "step": 5860
    },
    {
      "epoch": 0.6977986115231709,
      "grad_norm": 1.3370521068572998,
      "learning_rate": 6.234401761683387e-05,
      "loss": 0.3681,
      "step": 5880
    },
    {
      "epoch": 0.7001720761882158,
      "grad_norm": 3.5778584480285645,
      "learning_rate": 6.185466112062638e-05,
      "loss": 0.3834,
      "step": 5900
    },
    {
      "epoch": 0.7025455408532606,
      "grad_norm": 2.6569504737854004,
      "learning_rate": 6.136530462441889e-05,
      "loss": 0.3721,
      "step": 5920
    },
    {
      "epoch": 0.7049190055183053,
      "grad_norm": 2.0217671394348145,
      "learning_rate": 6.08759481282114e-05,
      "loss": 0.3484,
      "step": 5940
    },
    {
      "epoch": 0.7072924701833502,
      "grad_norm": 2.185582399368286,
      "learning_rate": 6.038659163200392e-05,
      "loss": 0.383,
      "step": 5960
    },
    {
      "epoch": 0.7096659348483949,
      "grad_norm": 7.005888938903809,
      "learning_rate": 5.9897235135796426e-05,
      "loss": 0.3813,
      "step": 5980
    },
    {
      "epoch": 0.7120393995134398,
      "grad_norm": 2.3591463565826416,
      "learning_rate": 5.9407878639588945e-05,
      "loss": 0.3703,
      "step": 6000
    },
    {
      "epoch": 0.7144128641784846,
      "grad_norm": 5.654657363891602,
      "learning_rate": 5.8918522143381463e-05,
      "loss": 0.3454,
      "step": 6020
    },
    {
      "epoch": 0.7167863288435293,
      "grad_norm": 1.2162609100341797,
      "learning_rate": 5.842916564717397e-05,
      "loss": 0.3766,
      "step": 6040
    },
    {
      "epoch": 0.7191597935085742,
      "grad_norm": 6.149990558624268,
      "learning_rate": 5.793980915096649e-05,
      "loss": 0.392,
      "step": 6060
    },
    {
      "epoch": 0.7215332581736189,
      "grad_norm": 1.9097468852996826,
      "learning_rate": 5.745045265475899e-05,
      "loss": 0.3475,
      "step": 6080
    },
    {
      "epoch": 0.7239067228386638,
      "grad_norm": 1.6493428945541382,
      "learning_rate": 5.6961096158551505e-05,
      "loss": 0.3529,
      "step": 6100
    },
    {
      "epoch": 0.7262801875037085,
      "grad_norm": 2.721097946166992,
      "learning_rate": 5.6471739662344023e-05,
      "loss": 0.3869,
      "step": 6120
    },
    {
      "epoch": 0.7286536521687533,
      "grad_norm": 13.59323501586914,
      "learning_rate": 5.598238316613653e-05,
      "loss": 0.4187,
      "step": 6140
    },
    {
      "epoch": 0.7310271168337982,
      "grad_norm": 4.005946159362793,
      "learning_rate": 5.549302666992905e-05,
      "loss": 0.3715,
      "step": 6160
    },
    {
      "epoch": 0.7334005814988429,
      "grad_norm": 10.959490776062012,
      "learning_rate": 5.500367017372155e-05,
      "loss": 0.3605,
      "step": 6180
    },
    {
      "epoch": 0.7357740461638878,
      "grad_norm": 3.7378504276275635,
      "learning_rate": 5.451431367751407e-05,
      "loss": 0.3746,
      "step": 6200
    },
    {
      "epoch": 0.7381475108289325,
      "grad_norm": 2.1223347187042236,
      "learning_rate": 5.402495718130658e-05,
      "loss": 0.3456,
      "step": 6220
    },
    {
      "epoch": 0.7405209754939773,
      "grad_norm": 3.748992443084717,
      "learning_rate": 5.3535600685099095e-05,
      "loss": 0.3411,
      "step": 6240
    },
    {
      "epoch": 0.7428944401590222,
      "grad_norm": 2.26949143409729,
      "learning_rate": 5.3046244188891614e-05,
      "loss": 0.3658,
      "step": 6260
    },
    {
      "epoch": 0.7452679048240669,
      "grad_norm": 1.5156259536743164,
      "learning_rate": 5.255688769268412e-05,
      "loss": 0.3557,
      "step": 6280
    },
    {
      "epoch": 0.7476413694891118,
      "grad_norm": 1.5241833925247192,
      "learning_rate": 5.206753119647664e-05,
      "loss": 0.3653,
      "step": 6300
    },
    {
      "epoch": 0.7500148341541565,
      "grad_norm": 2.4306817054748535,
      "learning_rate": 5.1578174700269143e-05,
      "loss": 0.374,
      "step": 6320
    },
    {
      "epoch": 0.7523882988192013,
      "grad_norm": 2.7033631801605225,
      "learning_rate": 5.108881820406166e-05,
      "loss": 0.3605,
      "step": 6340
    },
    {
      "epoch": 0.7547617634842462,
      "grad_norm": 1.6477036476135254,
      "learning_rate": 5.059946170785418e-05,
      "loss": 0.3596,
      "step": 6360
    },
    {
      "epoch": 0.7571352281492909,
      "grad_norm": 1.6728754043579102,
      "learning_rate": 5.0110105211646686e-05,
      "loss": 0.3759,
      "step": 6380
    },
    {
      "epoch": 0.7595086928143358,
      "grad_norm": 1.0842422246932983,
      "learning_rate": 4.9620748715439205e-05,
      "loss": 0.3778,
      "step": 6400
    },
    {
      "epoch": 0.7618821574793805,
      "grad_norm": 4.01175594329834,
      "learning_rate": 4.913139221923172e-05,
      "loss": 0.3606,
      "step": 6420
    },
    {
      "epoch": 0.7642556221444253,
      "grad_norm": 37.70307922363281,
      "learning_rate": 4.864203572302423e-05,
      "loss": 0.3508,
      "step": 6440
    },
    {
      "epoch": 0.7666290868094702,
      "grad_norm": 2.276616096496582,
      "learning_rate": 4.815267922681674e-05,
      "loss": 0.3587,
      "step": 6460
    },
    {
      "epoch": 0.7690025514745149,
      "grad_norm": 1.4267984628677368,
      "learning_rate": 4.766332273060925e-05,
      "loss": 0.3496,
      "step": 6480
    },
    {
      "epoch": 0.7713760161395598,
      "grad_norm": 0.9826087951660156,
      "learning_rate": 4.7173966234401765e-05,
      "loss": 0.3456,
      "step": 6500
    },
    {
      "epoch": 0.7737494808046045,
      "grad_norm": 2.8400752544403076,
      "learning_rate": 4.668460973819428e-05,
      "loss": 0.365,
      "step": 6520
    },
    {
      "epoch": 0.7761229454696493,
      "grad_norm": 1.2205694913864136,
      "learning_rate": 4.619525324198679e-05,
      "loss": 0.3511,
      "step": 6540
    },
    {
      "epoch": 0.7784964101346942,
      "grad_norm": 1.010634183883667,
      "learning_rate": 4.57058967457793e-05,
      "loss": 0.3532,
      "step": 6560
    },
    {
      "epoch": 0.7808698747997389,
      "grad_norm": 1.6635791063308716,
      "learning_rate": 4.521654024957181e-05,
      "loss": 0.3458,
      "step": 6580
    },
    {
      "epoch": 0.7832433394647837,
      "grad_norm": 1.0634078979492188,
      "learning_rate": 4.4727183753364325e-05,
      "loss": 0.3582,
      "step": 6600
    },
    {
      "epoch": 0.7856168041298285,
      "grad_norm": 1.944035530090332,
      "learning_rate": 4.423782725715684e-05,
      "loss": 0.3639,
      "step": 6620
    },
    {
      "epoch": 0.7879902687948733,
      "grad_norm": 1.266767978668213,
      "learning_rate": 4.3748470760949356e-05,
      "loss": 0.3503,
      "step": 6640
    },
    {
      "epoch": 0.7903637334599181,
      "grad_norm": 1.5824406147003174,
      "learning_rate": 4.325911426474187e-05,
      "loss": 0.3678,
      "step": 6660
    },
    {
      "epoch": 0.7927371981249629,
      "grad_norm": 1.49924635887146,
      "learning_rate": 4.276975776853438e-05,
      "loss": 0.3604,
      "step": 6680
    },
    {
      "epoch": 0.7951106627900077,
      "grad_norm": 2.785061836242676,
      "learning_rate": 4.228040127232689e-05,
      "loss": 0.3311,
      "step": 6700
    },
    {
      "epoch": 0.7974841274550525,
      "grad_norm": 3.4611728191375732,
      "learning_rate": 4.1791044776119404e-05,
      "loss": 0.3863,
      "step": 6720
    },
    {
      "epoch": 0.7998575921200973,
      "grad_norm": 4.345973491668701,
      "learning_rate": 4.1301688279911916e-05,
      "loss": 0.3911,
      "step": 6740
    },
    {
      "epoch": 0.8022310567851421,
      "grad_norm": 0.9219444990158081,
      "learning_rate": 4.0812331783704434e-05,
      "loss": 0.3688,
      "step": 6760
    },
    {
      "epoch": 0.8046045214501869,
      "grad_norm": 2.0534746646881104,
      "learning_rate": 4.0322975287496946e-05,
      "loss": 0.3487,
      "step": 6780
    },
    {
      "epoch": 0.8069779861152317,
      "grad_norm": 23.100229263305664,
      "learning_rate": 3.983361879128946e-05,
      "loss": 0.3658,
      "step": 6800
    },
    {
      "epoch": 0.8093514507802765,
      "grad_norm": 28.153846740722656,
      "learning_rate": 3.934426229508197e-05,
      "loss": 0.3594,
      "step": 6820
    },
    {
      "epoch": 0.8117249154453213,
      "grad_norm": 3.8349721431732178,
      "learning_rate": 3.885490579887448e-05,
      "loss": 0.3533,
      "step": 6840
    },
    {
      "epoch": 0.8140983801103661,
      "grad_norm": 1.965242862701416,
      "learning_rate": 3.8365549302666994e-05,
      "loss": 0.351,
      "step": 6860
    },
    {
      "epoch": 0.8164718447754109,
      "grad_norm": 1.3038301467895508,
      "learning_rate": 3.7876192806459506e-05,
      "loss": 0.3799,
      "step": 6880
    },
    {
      "epoch": 0.8188453094404557,
      "grad_norm": 2.0096659660339355,
      "learning_rate": 3.738683631025202e-05,
      "loss": 0.3634,
      "step": 6900
    },
    {
      "epoch": 0.8212187741055005,
      "grad_norm": 1.8686543703079224,
      "learning_rate": 3.689747981404453e-05,
      "loss": 0.3482,
      "step": 6920
    },
    {
      "epoch": 0.8235922387705453,
      "grad_norm": 2.402479887008667,
      "learning_rate": 3.640812331783704e-05,
      "loss": 0.3485,
      "step": 6940
    },
    {
      "epoch": 0.8259657034355901,
      "grad_norm": 2.238645076751709,
      "learning_rate": 3.5918766821629554e-05,
      "loss": 0.3347,
      "step": 6960
    },
    {
      "epoch": 0.8283391681006349,
      "grad_norm": 3.8820297718048096,
      "learning_rate": 3.542941032542207e-05,
      "loss": 0.3542,
      "step": 6980
    },
    {
      "epoch": 0.8307126327656797,
      "grad_norm": 4.045528888702393,
      "learning_rate": 3.4940053829214585e-05,
      "loss": 0.3556,
      "step": 7000
    },
    {
      "epoch": 0.8330860974307245,
      "grad_norm": 2.289517641067505,
      "learning_rate": 3.44506973330071e-05,
      "loss": 0.3563,
      "step": 7020
    },
    {
      "epoch": 0.8354595620957693,
      "grad_norm": 1.561686396598816,
      "learning_rate": 3.396134083679961e-05,
      "loss": 0.3631,
      "step": 7040
    },
    {
      "epoch": 0.8378330267608141,
      "grad_norm": 5.2924017906188965,
      "learning_rate": 3.347198434059212e-05,
      "loss": 0.3709,
      "step": 7060
    },
    {
      "epoch": 0.8402064914258589,
      "grad_norm": 5.051037788391113,
      "learning_rate": 3.298262784438463e-05,
      "loss": 0.386,
      "step": 7080
    },
    {
      "epoch": 0.8425799560909037,
      "grad_norm": 5.35546875,
      "learning_rate": 3.249327134817715e-05,
      "loss": 0.3248,
      "step": 7100
    },
    {
      "epoch": 0.8449534207559485,
      "grad_norm": 1.1158454418182373,
      "learning_rate": 3.2003914851969664e-05,
      "loss": 0.357,
      "step": 7120
    },
    {
      "epoch": 0.8473268854209933,
      "grad_norm": 5.548925876617432,
      "learning_rate": 3.1514558355762176e-05,
      "loss": 0.3449,
      "step": 7140
    },
    {
      "epoch": 0.8497003500860381,
      "grad_norm": 1.81381094455719,
      "learning_rate": 3.102520185955469e-05,
      "loss": 0.3743,
      "step": 7160
    },
    {
      "epoch": 0.8520738147510829,
      "grad_norm": 5.350663185119629,
      "learning_rate": 3.05358453633472e-05,
      "loss": 0.3796,
      "step": 7180
    },
    {
      "epoch": 0.8544472794161277,
      "grad_norm": 1.3412513732910156,
      "learning_rate": 3.0046488867139712e-05,
      "loss": 0.3502,
      "step": 7200
    },
    {
      "epoch": 0.8568207440811725,
      "grad_norm": 1.3861067295074463,
      "learning_rate": 2.9557132370932227e-05,
      "loss": 0.3614,
      "step": 7220
    },
    {
      "epoch": 0.8591942087462173,
      "grad_norm": 2.367455005645752,
      "learning_rate": 2.906777587472474e-05,
      "loss": 0.3392,
      "step": 7240
    },
    {
      "epoch": 0.8615676734112621,
      "grad_norm": 1.6966514587402344,
      "learning_rate": 2.857841937851725e-05,
      "loss": 0.3754,
      "step": 7260
    },
    {
      "epoch": 0.8639411380763069,
      "grad_norm": 6.9661664962768555,
      "learning_rate": 2.8089062882309763e-05,
      "loss": 0.3433,
      "step": 7280
    },
    {
      "epoch": 0.8663146027413516,
      "grad_norm": 1.1188935041427612,
      "learning_rate": 2.7599706386102275e-05,
      "loss": 0.3834,
      "step": 7300
    },
    {
      "epoch": 0.8686880674063965,
      "grad_norm": 2.010484218597412,
      "learning_rate": 2.7110349889894787e-05,
      "loss": 0.3552,
      "step": 7320
    },
    {
      "epoch": 0.8710615320714413,
      "grad_norm": 2.191795825958252,
      "learning_rate": 2.6620993393687306e-05,
      "loss": 0.3442,
      "step": 7340
    },
    {
      "epoch": 0.8734349967364861,
      "grad_norm": 1.7182886600494385,
      "learning_rate": 2.6131636897479818e-05,
      "loss": 0.3744,
      "step": 7360
    },
    {
      "epoch": 0.8758084614015309,
      "grad_norm": 2.151902437210083,
      "learning_rate": 2.5642280401272327e-05,
      "loss": 0.3552,
      "step": 7380
    },
    {
      "epoch": 0.8781819260665756,
      "grad_norm": 2.1001129150390625,
      "learning_rate": 2.515292390506484e-05,
      "loss": 0.3412,
      "step": 7400
    },
    {
      "epoch": 0.8805553907316205,
      "grad_norm": 2.045316696166992,
      "learning_rate": 2.4663567408857354e-05,
      "loss": 0.3975,
      "step": 7420
    },
    {
      "epoch": 0.8829288553966653,
      "grad_norm": 1.456080675125122,
      "learning_rate": 2.4174210912649866e-05,
      "loss": 0.3403,
      "step": 7440
    },
    {
      "epoch": 0.8853023200617101,
      "grad_norm": 1.2342464923858643,
      "learning_rate": 2.3684854416442378e-05,
      "loss": 0.345,
      "step": 7460
    },
    {
      "epoch": 0.8876757847267549,
      "grad_norm": 1.33303701877594,
      "learning_rate": 2.3195497920234893e-05,
      "loss": 0.3533,
      "step": 7480
    },
    {
      "epoch": 0.8900492493917996,
      "grad_norm": 3.7526323795318604,
      "learning_rate": 2.2706141424027405e-05,
      "loss": 0.3646,
      "step": 7500
    },
    {
      "epoch": 0.8924227140568445,
      "grad_norm": 1.8012620210647583,
      "learning_rate": 2.2216784927819917e-05,
      "loss": 0.3656,
      "step": 7520
    },
    {
      "epoch": 0.8947961787218893,
      "grad_norm": 1.3539458513259888,
      "learning_rate": 2.1727428431612433e-05,
      "loss": 0.3536,
      "step": 7540
    },
    {
      "epoch": 0.8971696433869341,
      "grad_norm": 1.1978185176849365,
      "learning_rate": 2.1238071935404945e-05,
      "loss": 0.3325,
      "step": 7560
    },
    {
      "epoch": 0.8995431080519789,
      "grad_norm": 1.4112329483032227,
      "learning_rate": 2.0748715439197457e-05,
      "loss": 0.3619,
      "step": 7580
    },
    {
      "epoch": 0.9019165727170236,
      "grad_norm": 2.64255690574646,
      "learning_rate": 2.025935894298997e-05,
      "loss": 0.3406,
      "step": 7600
    },
    {
      "epoch": 0.9042900373820685,
      "grad_norm": 2.6844842433929443,
      "learning_rate": 1.977000244678248e-05,
      "loss": 0.3586,
      "step": 7620
    },
    {
      "epoch": 0.9066635020471133,
      "grad_norm": 2.3552961349487305,
      "learning_rate": 1.9280645950574993e-05,
      "loss": 0.346,
      "step": 7640
    },
    {
      "epoch": 0.9090369667121581,
      "grad_norm": 13.580759048461914,
      "learning_rate": 1.8791289454367508e-05,
      "loss": 0.3402,
      "step": 7660
    },
    {
      "epoch": 0.9114104313772029,
      "grad_norm": 1.640359878540039,
      "learning_rate": 1.830193295816002e-05,
      "loss": 0.3666,
      "step": 7680
    },
    {
      "epoch": 0.9137838960422476,
      "grad_norm": 4.322233200073242,
      "learning_rate": 1.7812576461952532e-05,
      "loss": 0.3657,
      "step": 7700
    },
    {
      "epoch": 0.9161573607072925,
      "grad_norm": 2.0856335163116455,
      "learning_rate": 1.7323219965745048e-05,
      "loss": 0.3381,
      "step": 7720
    },
    {
      "epoch": 0.9185308253723373,
      "grad_norm": 1.3727226257324219,
      "learning_rate": 1.683386346953756e-05,
      "loss": 0.3561,
      "step": 7740
    },
    {
      "epoch": 0.9209042900373821,
      "grad_norm": 10.562019348144531,
      "learning_rate": 1.634450697333007e-05,
      "loss": 0.3613,
      "step": 7760
    },
    {
      "epoch": 0.9232777547024269,
      "grad_norm": 1.2633640766143799,
      "learning_rate": 1.5855150477122584e-05,
      "loss": 0.4033,
      "step": 7780
    },
    {
      "epoch": 0.9256512193674716,
      "grad_norm": 3.8921799659729004,
      "learning_rate": 1.5365793980915096e-05,
      "loss": 0.3517,
      "step": 7800
    },
    {
      "epoch": 0.9280246840325165,
      "grad_norm": 2.0634124279022217,
      "learning_rate": 1.4876437484707611e-05,
      "loss": 0.3399,
      "step": 7820
    },
    {
      "epoch": 0.9303981486975612,
      "grad_norm": 0.8247445225715637,
      "learning_rate": 1.4387080988500123e-05,
      "loss": 0.3314,
      "step": 7840
    },
    {
      "epoch": 0.9327716133626061,
      "grad_norm": 1.4403795003890991,
      "learning_rate": 1.3897724492292635e-05,
      "loss": 0.3708,
      "step": 7860
    },
    {
      "epoch": 0.9351450780276509,
      "grad_norm": 1.5721685886383057,
      "learning_rate": 1.340836799608515e-05,
      "loss": 0.363,
      "step": 7880
    },
    {
      "epoch": 0.9375185426926956,
      "grad_norm": 2.4135243892669678,
      "learning_rate": 1.2919011499877662e-05,
      "loss": 0.3522,
      "step": 7900
    },
    {
      "epoch": 0.9398920073577405,
      "grad_norm": 2.5609750747680664,
      "learning_rate": 1.2429655003670174e-05,
      "loss": 0.3599,
      "step": 7920
    },
    {
      "epoch": 0.9422654720227852,
      "grad_norm": 4.583984375,
      "learning_rate": 1.1940298507462686e-05,
      "loss": 0.3469,
      "step": 7940
    },
    {
      "epoch": 0.94463893668783,
      "grad_norm": 1.4812273979187012,
      "learning_rate": 1.14509420112552e-05,
      "loss": 0.3323,
      "step": 7960
    },
    {
      "epoch": 0.9470124013528749,
      "grad_norm": 1.4155384302139282,
      "learning_rate": 1.0961585515047714e-05,
      "loss": 0.3615,
      "step": 7980
    },
    {
      "epoch": 0.9493858660179196,
      "grad_norm": 0.9297001957893372,
      "learning_rate": 1.0472229018840226e-05,
      "loss": 0.3627,
      "step": 8000
    },
    {
      "epoch": 0.9493858660179196,
      "eval_f1_macro": 0.49997416185313626,
      "eval_f1_micro": 0.537771421141594,
      "eval_hamming_loss": 0.1422,
      "eval_loss": 0.3524816036224365,
      "eval_runtime": 457.5417,
      "eval_samples_per_second": 10.928,
      "eval_steps_per_second": 0.457,
      "eval_subset_accuracy": 0.4338,
      "step": 8000
    }
  ],
  "logging_steps": 20,
  "max_steps": 8427,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 8000,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.1385085983988e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
